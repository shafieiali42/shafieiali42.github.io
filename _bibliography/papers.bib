---
---

@string{aps = {American Physical Society,}}


@misc{dehghani2024emo3dmetricbenchmarkingdataset,
      title={Emo3D: Metric and Benchmarking Dataset for 3D Facial Expression Generation from Emotion Description}, 
      author={Mahshid Dehghani and Amirahmad Shafiee* and Ali Shafiei* and Neda Fallah and Farahmand Alizadeh and Mohammad Mehdi Gholinejad and Hamid Behroozi and Jafar Habibi and Ehsaneddin Asgari},
      year={2024},
      eprint={2410.02049},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2410.02049}, 
      preview={Emo3d_Dataset.png},
      selected={true},
      abbr={NAACL Findings},
      arXiv={2410.02049},
      abstract={Existing 3D facial emotion modeling have been constrained by limited emotion classes and insufficient datasets. This paper introduces "Emo3D", an extensive "Text-Image-Expression dataset" spanning a wide spectrum of human emotions, each paired with images and 3D blendshapes. Leveraging Large Language Models (LLMs), we generate a diverse array of textual descriptions, facilitating the capture of a broad spectrum of emotional expressions. Using this unique dataset, we conduct a comprehensive evaluation of language-based models' fine-tuning and vision-language models like Contranstive Language Image Pretraining (CLIP) for 3D facial expression synthesis. We also introduce a new evaluation metric for this task to more directly measure the conveyed emotion. Our new evaluation metric, Emo3D, demonstrates its superiority over Mean Squared Error (MSE) metrics in assessing visual-text alignment and semantic richness in 3D facial expressions associated with human emotions. "Emo3D" has great applications in animation design, virtual reality, and emotional human-computer interaction.}
}




